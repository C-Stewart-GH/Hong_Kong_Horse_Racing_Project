{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nathan Deinlein <br>\n",
    "Ryan Kinney <br>\n",
    "Chris Roche <br>\n",
    "Cameron Stewart <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 1 - Lab 2 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Preparation Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import plot_confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 1.1: Define and prepare your class variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set shape: (79423, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>race_id</th>\n",
       "      <th>horse_no</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>result</th>\n",
       "      <th>won</th>\n",
       "      <th>lengths_behind</th>\n",
       "      <th>horse_age</th>\n",
       "      <th>horse_country</th>\n",
       "      <th>horse_type</th>\n",
       "      <th>...</th>\n",
       "      <th>behind_sec2</th>\n",
       "      <th>behind_sec3</th>\n",
       "      <th>time1</th>\n",
       "      <th>time2</th>\n",
       "      <th>time3</th>\n",
       "      <th>finish_time</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>place_odds</th>\n",
       "      <th>trainer_id</th>\n",
       "      <th>jockey_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3917</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>3</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>13.85</td>\n",
       "      <td>21.59</td>\n",
       "      <td>23.86</td>\n",
       "      <td>83.92</td>\n",
       "      <td>9.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2157</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>14.57</td>\n",
       "      <td>21.99</td>\n",
       "      <td>23.30</td>\n",
       "      <td>83.56</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>164</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>858</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>13.69</td>\n",
       "      <td>21.59</td>\n",
       "      <td>23.90</td>\n",
       "      <td>83.40</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>137</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1853</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3</td>\n",
       "      <td>SAF</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>14.09</td>\n",
       "      <td>21.83</td>\n",
       "      <td>23.70</td>\n",
       "      <td>83.62</td>\n",
       "      <td>39.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>80</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2796</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3</td>\n",
       "      <td>GB</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>8.75</td>\n",
       "      <td>4.25</td>\n",
       "      <td>14.77</td>\n",
       "      <td>21.75</td>\n",
       "      <td>23.22</td>\n",
       "      <td>83.24</td>\n",
       "      <td>50.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  race_id  horse_no  horse_id  result  won  lengths_behind  \\\n",
       "0           0        0         1      3917      10  0.0            8.00   \n",
       "1           1        0         2      2157       8  0.0            5.75   \n",
       "2           2        0         3       858       7  0.0            4.75   \n",
       "3           3        0         4      1853       9  0.0            6.25   \n",
       "4           4        0         5      2796       6  0.0            3.75   \n",
       "\n",
       "   horse_age horse_country horse_type  ...  behind_sec2 behind_sec3  time1  \\\n",
       "0          3           AUS    Gelding  ...         2.00        1.50  13.85   \n",
       "1          3            NZ    Gelding  ...         9.00        5.00  14.57   \n",
       "2          3            NZ    Gelding  ...         1.00        0.75  13.69   \n",
       "3          3           SAF    Gelding  ...         5.00        3.50  14.09   \n",
       "4          3            GB    Gelding  ...         8.75        4.25  14.77   \n",
       "\n",
       "   time2  time3  finish_time  win_odds  place_odds  trainer_id  jockey_id  \n",
       "0  21.59  23.86        83.92       9.7         3.7         118          2  \n",
       "1  21.99  23.30        83.56      16.0         4.9         164         57  \n",
       "2  21.59  23.90        83.40       3.5         1.5         137         18  \n",
       "3  21.83  23.70        83.62      39.0        11.0          80         59  \n",
       "4  21.75  23.22        83.24      50.0        14.0           9        154  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data\n",
    "url = \"https://raw.githubusercontent.com/nedeinlein/Machine_Learning_I/main/runs_clean.csv\"\n",
    "runs_df = pd.read_csv(url, index_col=False)\n",
    "print('Data set shape:',runs_df.shape)\n",
    "runs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team's data set represents information collected from Hong Kong Horse Races. The data describes 29 race day attributes of 79,423 horses over 6,349 races (Note: Some horses are recorded in multiple races). Dates were obscured so timeframe of data is unknown. There are no missing values remaining in our imported data set. The raw data sets along with additional context can be located here: https://www.kaggle.com/gdaley/hkracing?select=runs.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Show Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>race_id</th>\n",
       "      <th>horse_no</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>result</th>\n",
       "      <th>won</th>\n",
       "      <th>lengths_behind</th>\n",
       "      <th>horse_age</th>\n",
       "      <th>horse_country</th>\n",
       "      <th>horse_type</th>\n",
       "      <th>...</th>\n",
       "      <th>behind_sec3</th>\n",
       "      <th>time1</th>\n",
       "      <th>time2</th>\n",
       "      <th>time3</th>\n",
       "      <th>finish_time</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>place_odds</th>\n",
       "      <th>trainer_id</th>\n",
       "      <th>jockey_id</th>\n",
       "      <th>show</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3917</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>3</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>1.50</td>\n",
       "      <td>13.85</td>\n",
       "      <td>21.59</td>\n",
       "      <td>23.86</td>\n",
       "      <td>83.92</td>\n",
       "      <td>9.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2157</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>14.57</td>\n",
       "      <td>21.99</td>\n",
       "      <td>23.30</td>\n",
       "      <td>83.56</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>164</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>858</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>13.69</td>\n",
       "      <td>21.59</td>\n",
       "      <td>23.90</td>\n",
       "      <td>83.40</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>137</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1853</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3</td>\n",
       "      <td>SAF</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>14.09</td>\n",
       "      <td>21.83</td>\n",
       "      <td>23.70</td>\n",
       "      <td>83.62</td>\n",
       "      <td>39.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>80</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2796</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3</td>\n",
       "      <td>GB</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>14.77</td>\n",
       "      <td>21.75</td>\n",
       "      <td>23.22</td>\n",
       "      <td>83.24</td>\n",
       "      <td>50.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  race_id  horse_no  horse_id  result  won  lengths_behind  \\\n",
       "0           0        0         1      3917      10  0.0            8.00   \n",
       "1           1        0         2      2157       8  0.0            5.75   \n",
       "2           2        0         3       858       7  0.0            4.75   \n",
       "3           3        0         4      1853       9  0.0            6.25   \n",
       "4           4        0         5      2796       6  0.0            3.75   \n",
       "\n",
       "   horse_age horse_country horse_type  ...  behind_sec3  time1  time2  time3  \\\n",
       "0          3           AUS    Gelding  ...         1.50  13.85  21.59  23.86   \n",
       "1          3            NZ    Gelding  ...         5.00  14.57  21.99  23.30   \n",
       "2          3            NZ    Gelding  ...         0.75  13.69  21.59  23.90   \n",
       "3          3           SAF    Gelding  ...         3.50  14.09  21.83  23.70   \n",
       "4          3            GB    Gelding  ...         4.25  14.77  21.75  23.22   \n",
       "\n",
       "   finish_time  win_odds  place_odds  trainer_id  jockey_id  show  \n",
       "0        83.92       9.7         3.7         118          2     0  \n",
       "1        83.56      16.0         4.9         164         57     0  \n",
       "2        83.40       3.5         1.5         137         18     0  \n",
       "3        83.62      39.0        11.0          80         59     0  \n",
       "4        83.24      50.0        14.0           9        154     0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a Show result feature\n",
    "runs_df['show'] = np.where(runs_df['result'] <= 3, 1, 0)\n",
    "runs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two variables our team is focusing on for classification are Won and Show. The Won variable is a binary 0 or 1 response to descibe if the horse placed first in the race. If the horse placed first in the race, the Won variable records a 1. The Show variable (created above) is also a binary 0 or 1 response that represents whether a horse placed first, second, or third in a race. If the horse does place in the top 3, then the Show variable records a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 1.2: Remove variables that are not needed/useful for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Non-Important Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor data set shape after dropping non-important predictors \n",
      "and separating response variables into their own data sets: \n",
      " (79423, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horse_no</th>\n",
       "      <th>horse_age</th>\n",
       "      <th>horse_country</th>\n",
       "      <th>horse_type</th>\n",
       "      <th>horse_rating</th>\n",
       "      <th>declared_weight</th>\n",
       "      <th>actual_weight</th>\n",
       "      <th>draw</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>place_odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>60</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>133</td>\n",
       "      <td>7</td>\n",
       "      <td>9.7</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>60</td>\n",
       "      <td>980.0</td>\n",
       "      <td>133</td>\n",
       "      <td>12</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>60</td>\n",
       "      <td>1082.0</td>\n",
       "      <td>132</td>\n",
       "      <td>8</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>SAF</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>60</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>127</td>\n",
       "      <td>13</td>\n",
       "      <td>39.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>GB</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>60</td>\n",
       "      <td>972.0</td>\n",
       "      <td>131</td>\n",
       "      <td>14</td>\n",
       "      <td>50.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   horse_no  horse_age horse_country horse_type  horse_rating  \\\n",
       "0         1          3           AUS    Gelding            60   \n",
       "1         2          3            NZ    Gelding            60   \n",
       "2         3          3            NZ    Gelding            60   \n",
       "3         4          3           SAF    Gelding            60   \n",
       "4         5          3            GB    Gelding            60   \n",
       "\n",
       "   declared_weight  actual_weight  draw  win_odds  place_odds  \n",
       "0           1020.0            133     7       9.7         3.7  \n",
       "1            980.0            133    12      16.0         4.9  \n",
       "2           1082.0            132     8       3.5         1.5  \n",
       "3           1118.0            127    13      39.0        11.0  \n",
       "4            972.0            131    14      50.0        14.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs_data = runs_df.drop(['Unnamed: 0','race_id','horse_id','result','won','lengths_behind','horse_gear','position_sec1','position_sec2','position_sec3','behind_sec1','behind_sec2','behind_sec3','time1','time2','time3','finish_time','trainer_id','jockey_id','show'], axis=1)\n",
    "runs_target = runs_df['won']\n",
    "runs_target2 = runs_df['show']\n",
    "print('Predictor data set shape after dropping non-important predictors \\nand separating response variables into their own data sets: \\n',runs_data.shape)\n",
    "runs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our objective is to predict the result before the race starts, we must drop any predictors that are not known pre-race. Due to this, we dropped predictors such as: how many lengths the horse finished behind the winner, lap times, lap position, finish time, and finish position.\n",
    "\n",
    "Also, we decided to drop any predictors that are not relevant to predicting the result of the race. These predictors include 'Unnamed: 0' (this was essentially an index value), the ID of the race, and the ID of the horse.\n",
    "\n",
    "Next, we dropped Trainer ID, Jockey ID, and Horse Gear because they each have hundreds to thousands of small sample categorical levels that do not help predict the result of the race.\n",
    "\n",
    "Finally, we separate out the response variables (Won and Show) into their own data sets. This leaves our predictor data set with a shape of 79423 instances and 8 attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 1.3: Use proper variable representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after One-Hot Encoding Categorical Predictors: (79423, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horse_no</th>\n",
       "      <th>horse_age</th>\n",
       "      <th>horse_rating</th>\n",
       "      <th>declared_weight</th>\n",
       "      <th>actual_weight</th>\n",
       "      <th>draw</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>place_odds</th>\n",
       "      <th>horse_country_ARG</th>\n",
       "      <th>horse_country_AUS</th>\n",
       "      <th>...</th>\n",
       "      <th>horse_country_ZIM</th>\n",
       "      <th>horse_type_Brown</th>\n",
       "      <th>horse_type_Colt</th>\n",
       "      <th>horse_type_Filly</th>\n",
       "      <th>horse_type_Gelding</th>\n",
       "      <th>horse_type_Grey</th>\n",
       "      <th>horse_type_Horse</th>\n",
       "      <th>horse_type_Mare</th>\n",
       "      <th>horse_type_Rig</th>\n",
       "      <th>horse_type_Roan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>133</td>\n",
       "      <td>7</td>\n",
       "      <td>9.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>980.0</td>\n",
       "      <td>133</td>\n",
       "      <td>12</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1082.0</td>\n",
       "      <td>132</td>\n",
       "      <td>8</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>127</td>\n",
       "      <td>13</td>\n",
       "      <td>39.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>972.0</td>\n",
       "      <td>131</td>\n",
       "      <td>14</td>\n",
       "      <td>50.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   horse_no  horse_age  horse_rating  declared_weight  actual_weight  draw  \\\n",
       "0         1          3            60           1020.0            133     7   \n",
       "1         2          3            60            980.0            133    12   \n",
       "2         3          3            60           1082.0            132     8   \n",
       "3         4          3            60           1118.0            127    13   \n",
       "4         5          3            60            972.0            131    14   \n",
       "\n",
       "   win_odds  place_odds  horse_country_ARG  horse_country_AUS  ...  \\\n",
       "0       9.7         3.7                  0                  1  ...   \n",
       "1      16.0         4.9                  0                  0  ...   \n",
       "2       3.5         1.5                  0                  0  ...   \n",
       "3      39.0        11.0                  0                  0  ...   \n",
       "4      50.0        14.0                  0                  0  ...   \n",
       "\n",
       "   horse_country_ZIM  horse_type_Brown  horse_type_Colt  horse_type_Filly  \\\n",
       "0                  0                 0                0                 0   \n",
       "1                  0                 0                0                 0   \n",
       "2                  0                 0                0                 0   \n",
       "3                  0                 0                0                 0   \n",
       "4                  0                 0                0                 0   \n",
       "\n",
       "   horse_type_Gelding  horse_type_Grey  horse_type_Horse  horse_type_Mare  \\\n",
       "0                   1                0                 0                0   \n",
       "1                   1                0                 0                0   \n",
       "2                   1                0                 0                0   \n",
       "3                   1                0                 0                0   \n",
       "4                   1                0                 0                0   \n",
       "\n",
       "   horse_type_Rig  horse_type_Roan  \n",
       "0               0                0  \n",
       "1               0                0  \n",
       "2               0                0  \n",
       "3               0                0  \n",
       "4               0                0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use one hot encoding on non-numerical features\n",
    "## (Then remove them from the drop code chunk below)\n",
    "# perform one-hot encoding of the categorical data \"embarked\"\n",
    "tmp_df = pd.get_dummies(runs_data.horse_country,prefix='horse_country')\n",
    "runs_df_onehot = pd.concat((runs_data,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(runs_data.horse_type,prefix='horse_type')\n",
    "runs_df_onehot = pd.concat((runs_df_onehot,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "runs_data = runs_df_onehot.reset_index(drop=True)\n",
    "runs_data = runs_data.drop(['horse_country','horse_type'], axis=1)\n",
    "\n",
    "print('Shape after One-Hot Encoding Categorical Predictors:',runs_data.shape)\n",
    "runs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data to the key predictors, we one-hot encode the remaining categorical variables horse country and horse type. This will change each categorical variable level into a binary 0 or 1 response. Then we drop the original attributes that were one-hot encoded. The number of attributes in the predictor data set increased from 10 to 33 after one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 1.4: Use pre-processing methods (as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completed the pre-processing our team performed in creating our predictor data set (runs_data), our 'Won' response data set (runs_target), and our 'Show' response data set (runs_target2). One additional pre-processing step not seen until the model creation in Section 5 is scaling. We built a scaling step into the pipeline that is used to scale, train, and fit the models. We used StandardScaler() and MinMaxScaler() in the models in Section 5 based on which was more appropriate. StandardScalar() scales each predictor based on using a standard normal distribution with a mean of 0 and standard deviation of 1. MinMaxScaler() scales each predictor so the values fall in the range [0,1]. When scaling the data set, we fit the scale to the training model and transform both the train and test sets based on the fit. We did this individually for each of the 10 cross validation splits. We will talk about the cross validation method chosen more in Section 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Preparation Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 2.1: Describe the final dataset that is used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Won Response Set Shape:  (79423,)\n",
      "Show Response Set Shape:  (79423,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 79423 entries, 0 to 79422\n",
      "Data columns (total 33 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   horse_no            79423 non-null  int64  \n",
      " 1   horse_age           79423 non-null  int64  \n",
      " 2   horse_rating        79423 non-null  int64  \n",
      " 3   declared_weight     79423 non-null  float64\n",
      " 4   actual_weight       79423 non-null  int64  \n",
      " 5   draw                79423 non-null  int64  \n",
      " 6   win_odds            79423 non-null  float64\n",
      " 7   place_odds          79423 non-null  float64\n",
      " 8   horse_country_ARG   79423 non-null  uint8  \n",
      " 9   horse_country_AUS   79423 non-null  uint8  \n",
      " 10  horse_country_BRZ   79423 non-null  uint8  \n",
      " 11  horse_country_CAN   79423 non-null  uint8  \n",
      " 12  horse_country_FR    79423 non-null  uint8  \n",
      " 13  horse_country_GB    79423 non-null  uint8  \n",
      " 14  horse_country_GER   79423 non-null  uint8  \n",
      " 15  horse_country_GR    79423 non-null  uint8  \n",
      " 16  horse_country_IRE   79423 non-null  uint8  \n",
      " 17  horse_country_ITY   79423 non-null  uint8  \n",
      " 18  horse_country_JPN   79423 non-null  uint8  \n",
      " 19  horse_country_NZ    79423 non-null  uint8  \n",
      " 20  horse_country_SAF   79423 non-null  uint8  \n",
      " 21  horse_country_SPA   79423 non-null  uint8  \n",
      " 22  horse_country_USA   79423 non-null  uint8  \n",
      " 23  horse_country_ZIM   79423 non-null  uint8  \n",
      " 24  horse_type_Brown    79423 non-null  uint8  \n",
      " 25  horse_type_Colt     79423 non-null  uint8  \n",
      " 26  horse_type_Filly    79423 non-null  uint8  \n",
      " 27  horse_type_Gelding  79423 non-null  uint8  \n",
      " 28  horse_type_Grey     79423 non-null  uint8  \n",
      " 29  horse_type_Horse    79423 non-null  uint8  \n",
      " 30  horse_type_Mare     79423 non-null  uint8  \n",
      " 31  horse_type_Rig      79423 non-null  uint8  \n",
      " 32  horse_type_Roan     79423 non-null  uint8  \n",
      "dtypes: float64(3), int64(5), uint8(25)\n",
      "memory usage: 6.7 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horse_no</th>\n",
       "      <th>horse_age</th>\n",
       "      <th>horse_rating</th>\n",
       "      <th>declared_weight</th>\n",
       "      <th>actual_weight</th>\n",
       "      <th>draw</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>place_odds</th>\n",
       "      <th>horse_country_ARG</th>\n",
       "      <th>horse_country_AUS</th>\n",
       "      <th>...</th>\n",
       "      <th>horse_country_ZIM</th>\n",
       "      <th>horse_type_Brown</th>\n",
       "      <th>horse_type_Colt</th>\n",
       "      <th>horse_type_Filly</th>\n",
       "      <th>horse_type_Gelding</th>\n",
       "      <th>horse_type_Grey</th>\n",
       "      <th>horse_type_Horse</th>\n",
       "      <th>horse_type_Mare</th>\n",
       "      <th>horse_type_Rig</th>\n",
       "      <th>horse_type_Roan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "      <td>79423.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.905644</td>\n",
       "      <td>3.339461</td>\n",
       "      <td>61.035103</td>\n",
       "      <td>1104.952609</td>\n",
       "      <td>122.730091</td>\n",
       "      <td>6.875729</td>\n",
       "      <td>28.806138</td>\n",
       "      <td>7.421841</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.376415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.025736</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.950367</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.013762</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.000604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.760600</td>\n",
       "      <td>0.876864</td>\n",
       "      <td>11.750531</td>\n",
       "      <td>62.350596</td>\n",
       "      <td>6.305551</td>\n",
       "      <td>3.747588</td>\n",
       "      <td>30.093854</td>\n",
       "      <td>8.613459</td>\n",
       "      <td>0.039956</td>\n",
       "      <td>0.484489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012291</td>\n",
       "      <td>0.158346</td>\n",
       "      <td>0.061851</td>\n",
       "      <td>0.023262</td>\n",
       "      <td>0.217187</td>\n",
       "      <td>0.014629</td>\n",
       "      <td>0.116501</td>\n",
       "      <td>0.054660</td>\n",
       "      <td>0.043991</td>\n",
       "      <td>0.024576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>693.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1062.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1102.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1146.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>1369.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           horse_no     horse_age  horse_rating  declared_weight  \\\n",
       "count  79423.000000  79423.000000  79423.000000     79423.000000   \n",
       "mean       6.905644      3.339461     61.035103      1104.952609   \n",
       "std        3.760600      0.876864     11.750531        62.350596   \n",
       "min        1.000000      2.000000     10.000000       693.000000   \n",
       "25%        4.000000      3.000000     60.000000      1062.000000   \n",
       "50%        7.000000      3.000000     60.000000      1102.000000   \n",
       "75%       10.000000      3.000000     60.000000      1146.000000   \n",
       "max       14.000000     10.000000    138.000000      1369.000000   \n",
       "\n",
       "       actual_weight          draw      win_odds    place_odds  \\\n",
       "count   79423.000000  79423.000000  79423.000000  79423.000000   \n",
       "mean      122.730091      6.875729     28.806138      7.421841   \n",
       "std         6.305551      3.747588     30.093854      8.613459   \n",
       "min       103.000000      1.000000      1.000000      1.000000   \n",
       "25%       118.000000      4.000000      7.700000      2.400000   \n",
       "50%       123.000000      7.000000     15.000000      4.400000   \n",
       "75%       128.000000     10.000000     38.000000      8.200000   \n",
       "max       133.000000     15.000000     99.000000    101.000000   \n",
       "\n",
       "       horse_country_ARG  horse_country_AUS  ...  horse_country_ZIM  \\\n",
       "count       79423.000000       79423.000000  ...       79423.000000   \n",
       "mean            0.001599           0.376415  ...           0.000151   \n",
       "std             0.039956           0.484489  ...           0.012291   \n",
       "min             0.000000           0.000000  ...           0.000000   \n",
       "25%             0.000000           0.000000  ...           0.000000   \n",
       "50%             0.000000           0.000000  ...           0.000000   \n",
       "75%             0.000000           1.000000  ...           0.000000   \n",
       "max             1.000000           1.000000  ...           1.000000   \n",
       "\n",
       "       horse_type_Brown  horse_type_Colt  horse_type_Filly  \\\n",
       "count      79423.000000     79423.000000      79423.000000   \n",
       "mean           0.025736         0.003840          0.000541   \n",
       "std            0.158346         0.061851          0.023262   \n",
       "min            0.000000         0.000000          0.000000   \n",
       "25%            0.000000         0.000000          0.000000   \n",
       "50%            0.000000         0.000000          0.000000   \n",
       "75%            0.000000         0.000000          0.000000   \n",
       "max            1.000000         1.000000          1.000000   \n",
       "\n",
       "       horse_type_Gelding  horse_type_Grey  horse_type_Horse  horse_type_Mare  \\\n",
       "count        79423.000000     79423.000000      79423.000000     79423.000000   \n",
       "mean             0.950367         0.000214          0.013762         0.002997   \n",
       "std              0.217187         0.014629          0.116501         0.054660   \n",
       "min              0.000000         0.000000          0.000000         0.000000   \n",
       "25%              1.000000         0.000000          0.000000         0.000000   \n",
       "50%              1.000000         0.000000          0.000000         0.000000   \n",
       "75%              1.000000         0.000000          0.000000         0.000000   \n",
       "max              1.000000         1.000000          1.000000         1.000000   \n",
       "\n",
       "       horse_type_Rig  horse_type_Roan  \n",
       "count    79423.000000     79423.000000  \n",
       "mean         0.001939         0.000604  \n",
       "std          0.043991         0.024576  \n",
       "min          0.000000         0.000000  \n",
       "25%          0.000000         0.000000  \n",
       "50%          0.000000         0.000000  \n",
       "75%          0.000000         0.000000  \n",
       "max          1.000000         1.000000  \n",
       "\n",
       "[8 rows x 33 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Won Response Set Shape: ',runs_target.shape)\n",
    "print('Show Response Set Shape: ',runs_target2.shape)\n",
    "runs_data.info()\n",
    "runs_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final predictor data set contains 33 attributes and 79,423 instances. The data set contains 8 coninuous attributes and 25 binary attributes created from one-hot encoding. The response sets for Won and Show each contain 79,423 instances as well. The descriptive statistics for each predictor are listed above. You can see that all one-hot encoded predictors have a min of 0 and max of 1 since these are binary variables. All other continuous variables do not appear to have any concerning outliers when looking at the min and max values.\n",
    "\n",
    "Remaining original continuous features contained in the runs_data set:\n",
    "- Horse_no - the number assigned to this horse, in the race\n",
    "- Horse_age: all horses in the data set are 2, 3, 4, 5, 6, 7, 8, 9, or 10 years old at the time of race. The average age of horses in the dataset is 3.3 years\n",
    "- Horse_rating - rating number assigned by HKJC to this horse at the time of the race\n",
    "- Declared_weight: floating point value for the weight of the horse, the jockey, and the equipment in pounds.\n",
    "- Actual_weight: integer value for the weight the horse carried; i.e. the weight of the Jockey and equipment\n",
    "- Draw - post position number of the horse in this race\n",
    "- Win_odds: floating point value for the odds of the horse to win the race\n",
    "- Place_odds: floating point value for the odds of the horse to place in 1st, 2nd, or 3rd\n",
    "\n",
    "Remaining original categorical features contained in the runs_data set that were one-hot encoded:\n",
    "- Horse_country: a country code for the country of origin for the horse. Examples are NZ, AUS, USA\n",
    "- Horse_type: examples are gelding, mare, rig\n",
    "\n",
    "Response Variables:\n",
    "- Won: a boolean value where 1 indicates the horse finished first in the race and 0 indicates the horse did not. A horse with a Result of 1 will have a Won response of 1. All others will have a Won response of 0.\n",
    "- Show: a boolean value where 1 indicates the horse placed in the top 3 of the race and 0 indicates the horse did not. A horse with a Result of 1, 2, or 3 will have a Show response of 1. All others will have a Show response of 0.\n",
    "\n",
    "*Show is the one new variable created from the original set*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Modeling and Evaluation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t• Modeling and Evaluation 1\n",
    "\t\t○ Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "\t\tprecision, recall, F-measure, or any metric we have discussed).\n",
    "\t\t\t- Why are the measure(s) appropriate for analyzing the results of your modeling?\n",
    "\t\t\t- Give a detailed explanation backing up any assertions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 3.1: Choose and explain your evaluation metrics that you will use. (Give a detailed explanation backing up any assertions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What evaluation metrics will our team use? Why are the measure(s) appropriate for analyzing the results of your modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real life application of predicting Won and Show would be from the perspective of someone placing a wager for that one of the events will occur. The models will tell a better to place a wager only when that event is predicted to occur. This means that the better will only lose money when a False Positive occurs and will not lose any money from a False Negative. For this reason, our team is focused on maximizing precision.\n",
    "\n",
    "The one downfall of focusing on precision alone would be that a model could accurately predict a very small number of positive predictions and be optimal. Having too small of a number of positive predicitions would not give much opportunity to a better to make money. A secondary metric we are observing is Area Under the Curve (AUC). AUC score provides a good balance between sensitivity and specificity. A significantly higher AUC by two models with similar precision signals that there are a better balance of predictions.\n",
    "\n",
    "Since 92% of the observations in the data set are losers, creating a model that predicts a loss for every observation would have an accuracy of 92% but offer no practical use. Therefore, we are not using accuracy to select a model. We will display it as a reference for each of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Modeling and Evaluation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 4.1: Choose the method you will use for dividing your data into training and testing splits. Explain why your chosen method is appropriate or use more than one method as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dividing our test and train data sets, we used Stratified Shuffle Split Cross Validation (CV) with 10 splits with 80%/20% Train/Test sizes. Traditional Shuffle Split CV will randomly distribute each instance of a data set into a test or train set based on a inputted train/test percentage (we used 80%/20% Train/Test sizes). After distributing every instance between the train and test sets, we have created a split. Our team decided to create 10 splits which results in 10 train and 10 test sets. Due to each of our responses being highly imbalanced, we decided to use the special Stratified version of the Shuffle Split CV. The Stratified version ensures the responses in each split are as close to the same balance as possible (+/- 1 instance). This will ensure the model will have sufficient positive value to train on and we can see how well the model actually performs with sufficient postive values in the test set. \n",
    "\n",
    "Using this CV method will allow us to capture the mean Precision and AUC from 10 different models. These 10 models are trained and tested on 10 randomly generated train/test sets with nearly identical response class balance. This method will make it much more likely the most generalizable model will have the best scoring metrics compared to a single test/train split. Using a single test/train split will make it difficult to determine if your model is overfitting the data in that paticular random test/train split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Modeling and Evaluation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t• Modeling and Evaluation 3\n",
    "\t\t○ Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM\n",
    "        for task one and the same or different algorithms for task two). \n",
    "\t\t○ Two modeling techniques must be new (but the third could be SVM or logistic regression). \n",
    "\t\t○ Adjust parameters as appropriate to increase generalization performance using your chosen metric. \n",
    "\t\t○ You must investigate different parameters of the algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 5.1: Create three different classification/regression models for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training test split for showing confusion matrix\n",
    "# Note all evaluation will actually use kFold CrossValidation\n",
    "\n",
    "#train test split won\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(runs_data,runs_target,test_size=0.20,random_state=0)\n",
    "mmscl_obj = MinMaxScaler()\n",
    "X_train = mmscl_obj.fit_transform(X_train,y = None)\n",
    "X_test = mmscl_obj.transform(X_test)\n",
    "\n",
    "#train test split place\n",
    "X_train_place,X_test_place,Y_train_place,Y_test_place = train_test_split(runs_data,runs_target2,test_size=0.20,random_state=0)\n",
    "mmscl_obj = MinMaxScaler()\n",
    "X_train_place = mmscl_obj.fit_transform(X_train_place,y = None)\n",
    "X_test_place = mmscl_obj.transform(X_test_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 5.1.1: Model 1 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors is an algorithm frequently used for classification. In order to classify an observation, the algorithm measures the distance from the given observation to it's K nearest neighbors, where K is a tunable parameter.\n",
    "\n",
    "For each iteration of the model fitting, we try a different K value and print out the model precision and AUC score. Because the data set is not balanced, we use AUC as the primary metric for comparing model performance. If we used accuracy, we could achieve an accuracy of over 90% simply by classifying every observation as a \"loss\", but this has no practical use so we use Precision as a secondary metric instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we implemented two models. Both use Euclidean distance but one uses a uniform weight and the other uses distance. For uniform weight, all the nearest neighbors have the same impact in classification. In distance, closer neighbors have more impact.\n",
    "\n",
    "The difference in model performance for uniform vs. distance was negligable, but the model with weights='distance' trained more quickly so we elected to use that parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Distance, Uniform Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of classifier with 3 neighbors is: 0.189  AUC: 0.523\n",
      "Precision of classifier with 5 neighbors is: 0.230  AUC: 0.514\n",
      "Precision of classifier with 7 neighbors is: 0.274  AUC: 0.510\n",
      "Precision of classifier with 9 neighbors is: 0.250  AUC: 0.505\n",
      "Precision of classifier with 11 neighbors is: 0.298  AUC: 0.504\n",
      "Precision of classifier with 13 neighbors is: 0.348  AUC: 0.503\n",
      "Precision of classifier with 15 neighbors is: 0.350  AUC: 0.502\n",
      "Precision of classifier with 17 neighbors is: 0.231  AUC: 0.501\n"
     ]
    }
   ],
   "source": [
    "# Use Euclidean distance and iterate over several K-values\n",
    "## ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "Kvals = [3,5,7,9,11,13,15,17]\n",
    "for x in Kvals:\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=x, weights='uniform', metric='euclidean')\n",
    "    clf_knn.fit(X_train,Y_train)\n",
    "    yhat= clf_knn.predict(X_test)\n",
    "    acc = mt.accuracy_score(Y_test,yhat)\n",
    "    prec = mt.precision_score(Y_test,yhat)\n",
    "    auc = roc_auc_score(Y_test,yhat)\n",
    "    conf = mt.confusion_matrix(Y_test,yhat)\n",
    "    print('Precision of classifier with %d neighbors is: %.3f'%(x,prec), ' AUC: %.3f'%auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Distance, Distance Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of classifier with 3 neighbors is: 0.184  AUC: 0.522\n",
      "Precision of classifier with 5 neighbors is: 0.218  AUC: 0.514\n",
      "Precision of classifier with 7 neighbors is: 0.268  AUC: 0.511\n",
      "Precision of classifier with 9 neighbors is: 0.247  AUC: 0.506\n",
      "Precision of classifier with 11 neighbors is: 0.255  AUC: 0.504\n",
      "Precision of classifier with 13 neighbors is: 0.321  AUC: 0.503\n",
      "Precision of classifier with 15 neighbors is: 0.320  AUC: 0.503\n",
      "Precision of classifier with 17 neighbors is: 0.125  AUC: 0.500\n"
     ]
    }
   ],
   "source": [
    "# Use Euclidean distance - sqrt(sum((x - y)^2))\n",
    "## ‘distance’ : weight points by the inverse of their distance. \n",
    "##    in this case, closer neighbors of a query point will have a greater \n",
    "##    influence than neighbors which are further away.\n",
    "\n",
    "for x in Kvals:\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=x, weights='distance', metric='euclidean')\n",
    "    clf_knn.fit(X_train,Y_train)\n",
    "    yhat= clf_knn.predict(X_test)\n",
    "    acc = mt.accuracy_score(Y_test,yhat)\n",
    "    prec = mt.precision_score(Y_test,yhat)\n",
    "    auc = roc_auc_score(Y_test,yhat)\n",
    "    conf = mt.confusion_matrix(Y_test,yhat)\n",
    "    print('Precision of classifier with %d neighbors is: %.3f'%(x,prec), ' AUC: %.3f'%auc)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next parameter we experimented with was the weight metric. We re-ran the model several times using different metrics from the sklearn DistanceMetric library, to include Manhatten and Chebyshev.\n",
    "\n",
    "Again, the model performance for the different metrics was negligable so we elected to use weights = distance since it is well optimized for large data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Trees = 3\n",
      "   Mean Precision:  0.1604947932004665\n",
      "   Mean AUC:  0.5759630516567691\n",
      "   Mean Accuracy:  0.9000164037059187\n",
      "   Mean Time per Split:  14.240894746780395\n",
      " \n",
      "When Trees = 5\n",
      "   Mean Precision:  0.17242893921665783\n",
      "   Mean AUC:  0.6040195932809418\n",
      "   Mean Accuracy:  0.9108570467121682\n",
      "   Mean Time per Split:  21.529864978790282\n",
      " \n",
      "When Trees = 7\n",
      "   Mean Precision:  0.19674188127726275\n",
      "   Mean AUC:  0.6257389755284041\n",
      "   Mean Accuracy:  0.9153771712598575\n",
      "   Mean Time per Split:  24.91547155380249\n",
      " \n",
      "When Trees = 9\n",
      "   Mean Precision:  0.23136980840990629\n",
      "   Mean AUC:  0.6404224291268119\n",
      "   Mean Accuracy:  0.9176812895633593\n",
      "   Mean Time per Split:  24.753822779655454\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Use Euclidean Distance\n",
    "\n",
    "for x in Kvals:\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', KNeighborsClassifier(n_neighbors=x, weights='distance', metric='euclidean')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedKFold(n_splits = 10, shuffle = True ,random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Trees =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manhatten Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Manhatten distance - sum(|x - y|)\n",
    "\n",
    "for x in Kvals:\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', KNeighborsClassifier(n_neighbors=x, weights='distance', metric='manhattan')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedKFold(n_splits = 10, shuffle = True ,random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Trees =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chebyshev Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ChebyshevDistance distance - max(|x - y|)\n",
    "\n",
    "for x in Kvals:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', KNeighborsClassifier(n_neighbors=x, weights='distance', metric='chebyshev')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits = 10, shuffle = True ,random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Trees =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each of the KNN models above, the precision and AUC tend to stabilize at around k=9 or 11. Note as the K value increases so too does the precision but the AUC decreases. Choosing a smaller K-value such as 7 produces a good balace between the different metrics.\n",
    "\n",
    "We fit a final KNN using the parameters selected: Euclidean, Distance weights for neighbors, and 7 neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with Euclidean, Distance weight, and K=7 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on above, Euclidean with K=7 is a good combination of precision and AUC\n",
    "x=7\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=x, weights='distance', metric='euclidean')\n",
    "clf_knn.fit(X_train,Y_train)\n",
    "yhat= clf_knn.predict(X_test)\n",
    "acc = mt.accuracy_score(Y_test,yhat)\n",
    "prec = mt.precision_score(Y_test,yhat)\n",
    "auc = roc_auc_score(Y_test,yhat)\n",
    "conf = mt.confusion_matrix(Y_test,yhat)\n",
    "print('Precision of classifier with %d neighbors is: %.3f'%(x,prec), ' AUC: %.3f'%auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the Confusion Matrix plotted for the model. As can be seen, it classified the majority of observations as loses (correctly). This makes sense since about 92% of the data set are horses who did not win their race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix for KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf_knn, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Confusion Matrix above, notice the large number of false positives. The model incorrectly classified over 1000 observations as wins when they should have been loses. It only correctly classified about 50 wins. Since this data set is results of horse races, you can assume an interested party would be a gambler. The KNN model would not be of much benefit to a gambler due to the large number of Type I error in proportion to true positive results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Show Horses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we generated several more KNN models where we predicted Show (top 3 finish) instead of Win. We tried several different metrics of measure and K values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean distance for Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Euclidean Distance\n",
    "\n",
    "for x in Kvals:\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', KNeighborsClassifier(n_neighbors=x, weights='distance', metric='euclidean')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedKFold(n_splits = 10, shuffle = True ,random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target2, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Trees =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manhattan distance for Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Manhatten distance - sum(|x - y|)\n",
    "\n",
    "for x in Kvals:\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', KNeighborsClassifier(n_neighbors=x, weights='distance', metric='manhattan')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedKFold(n_splits = 10, shuffle = True ,random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target2, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Trees =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chebyshev distance for Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ChebyshevDistance distance - max(|x - y|)\n",
    "\n",
    "for x in Kvals:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', KNeighborsClassifier(n_neighbors=x, weights='distance', metric='chebyshev')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits = 10, shuffle = True ,random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target2, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Trees =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the different KNN models created to classify Show horses, the AUCs were all comparable to that of the KNN models for Win horses: slightly more than 0.5\n",
    "\n",
    "However, the precision was lower almost across the board. If I had to select one model, it would be the Euclidean distance metric with a K value of 13, although there a better model choices than KNN as we will see in the following sections of this Lab. In fact, in the Mini Lab we generated several Logistic Regression and SVM with significantly higher AUCs and precisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=7\n",
    "clf_knnw = KNeighborsClassifier(n_neighbors=7, weights='distance', metric='euclidean')\n",
    "clf_knnw.fit(X_train,Y_train)\n",
    "yhat= clf_knnw.predict(X_test)\n",
    "acc = mt.accuracy_score(Y_test,yhat)\n",
    "prec = mt.precision_score(Y_test,yhat)\n",
    "auc = roc_auc_score(Y_test,yhat)\n",
    "conf = mt.confusion_matrix(Y_test,yhat)\n",
    "print('Precision of classifier with %d neighbors is: %.3f'%(x,prec), ' AUC: %.3f'%auc)\n",
    "plot_confusion_matrix(clf_knn, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=13\n",
    "clf_knnw = KNeighborsClassifier(n_neighbors=7, weights='distance', metric='euclidean')\n",
    "clf_knnw.fit(X_train_place,Y_train_place)\n",
    "yhat= clf_knnw.predict(X_test_place)\n",
    "acc = mt.accuracy_score(Y_test_place,yhat)\n",
    "prec = mt.precision_score(Y_test_place,yhat)\n",
    "auc = roc_auc_score(Y_test_place,yhat)\n",
    "conf = mt.confusion_matrix(Y_test_place,yhat)\n",
    "print('Precision of classifier with %d neighbors is: %.3f'%(x,prec), ' AUC: %.3f'%auc)\n",
    "plot_confusion_matrix(clf_knn, X_test_place, Y_test_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 5.1.2: Model 2 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes on the 'won' target response variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var = [.25,.5,.75,.95,1,1.2,1.4,1.6,2]\n",
    "for x in var:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', GaussianNB(var_smoothing=x)))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When x =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complement Naive Bayes on the 'Won' target response variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var2 = [.05,.25,1,3,5,6]\n",
    "for x in var2:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', MinMaxScaler()))\n",
    "    steps.append(('model', ComplementNB(alpha = x)))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When x =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes on the 'show' target response variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var3 = [.25,.5,.75,.95,1,1.2,1.4,1.6,2]\n",
    "for x in var3:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', GaussianNB(var_smoothing=x)))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target2, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When x =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complement Naive Bayes on the 'show' target response variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var4 = [.001,1,5,20,30,50]\n",
    "for x in var4:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', MinMaxScaler()))\n",
    "    steps.append(('model', ComplementNB(alpha = x)))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target2, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When x =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will want to look at the feature importance using the Gaussian Naive Bayes model using the 'Won' feature to see if there are any features that could be left out to try to remove noise from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(runs_data.columns.values)\n",
    "\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "imps = permutation_importance(GNBmodel, X_test, Y_test)\n",
    "importances = imps.importances_mean\n",
    "std = imps.importances_std\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "\n",
    "featureorder = []\n",
    "for f in range(X_test.shape[1]):\n",
    "    featureorder.append(features[indices[f]])\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_test.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_test.shape[1]), [features[indices[i]] for i in range(33)],rotation = 90)\n",
    "plt.xlim([-1, X_test.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trying out a grid search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "gs_NB = GridSearchCV(estimator=nb_classifier, \n",
    "                 param_grid=params_NB, \n",
    "                 cv=StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0),   # use any cross validation technique \n",
    "                 verbose=1, \n",
    "                 scoring='precision') \n",
    "gs_NB.fit(X_train_Show, Y_train_Show)\n",
    "\n",
    "gs_NB.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SMOTE sampling technique on the imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample using SMOTE\n",
    "# Make all classes the same size as the majority class.\n",
    "sm = SMOTE(sampling_strategy='not majority', random_state = 0)\n",
    "X_sm, y_sm = sm.fit_resample(runs_data, runs_target)\n",
    "\n",
    "print('Resampled dataset shape %s' % Counter(y_sm))\n",
    "# Plot the new class distributions for y using the same funnction as above. \n",
    "def gini_index(y):\n",
    "    probs = pd.value_counts(y,normalize=True)\n",
    "    return 1 - np.sum(np.square(probs))\n",
    "\n",
    "def plot_class_dist(y):\n",
    "    fig, axarr = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    class_ct = len(np.unique(y))\n",
    "    vc = pd.value_counts(y)\n",
    "    print('Total Records', len(y))\n",
    "    print('Total Classes:', class_ct)\n",
    "    print('Class Gini Index', gini_index(runs_target))\n",
    "    print('Smallest Class Id:',vc.idxmin(),'Records:',vc.min())\n",
    "    print('Largest Class Id:',vc.idxmax(),'Records:',vc.max())\n",
    "    print('Accuracy when Guessing:', np.round( (1 / len(np.unique(y))) * 100, 2), '%')\n",
    "\n",
    "    sns.distplot(y, ax=axarr[0], bins=class_ct).set_title('LFW Class Distribution');\n",
    "    sns.distplot(y, ax=axarr[1], kde=False, bins=class_ct).set_title('LFW Class Counts');\n",
    "plot_class_dist(y_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def stratified_cross_validate(model, X, y, cv=cv):\n",
    "    start = time.time()\n",
    "    cv_results = cross_validate(model, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "    elapsed_time = (time.time() - start) \n",
    "    print ('Fold Scores:')\n",
    "    print(' ')\n",
    "    print(cv_results['test_score'])\n",
    "    print(' ')\n",
    "    print('Mean Accuracy: ', cv_results['test_score'].mean())\n",
    "    print('Mean Fit Time: ', cv_results['fit_time'].mean())\n",
    "    print('Mean Score Time: ', cv_results['score_time'].mean())\n",
    "    print('CV Time: ', elapsed_time)\n",
    "    return\n",
    "\n",
    "GNB = GaussianNB()\n",
    "stratified_cross_validate(GNB, X_sm, y_sm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 5.1.3: Model 3 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "The first parameter we want to look at is the performance of forest size to find an optimal number of trees based on the initial data. Determining the correct size of the forest can help keep us from overfitting the model or by wasting resources with building an unneccessarily large forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = [64,75,100,128]\n",
    "for x in trees:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', RandomForestClassifier(max_depth = 5, n_estimators = x, random_state = 0, class_weight = 'balanced')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedKFold(n_splits = 10, shuffle = True ,random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Trees =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this information we are aiming for the best precision in our models as we want the best possible number of positive results while limiting false positives. So from this we would look at building a forest with 64 trees. This is also nice because it is the least time consuming. Next we will use this to run a loop to fine the optimal max_depth. This is the metric that determines the overall size of each individual tree by limiting the number of levels it can break down the data into. We will be looking at four levels in this loop, 5,20,100, and no limit(none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = [5,20,100,None]\n",
    "for x in depth:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', RandomForestClassifier(max_depth = x, n_estimators = 64, random_state = 0, class_weight = 'balanced')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Depth =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assessing these results we wanted to take a look at feature importance on the selected model above. The reason for this is we want to try to eliminate noise variables. This has a secondary side effect with random forests of letting them run faster since the number of features is reduces. There were two ways we went about doing this for random forest. The first was a manual look at the feature importance, and selecting what we deemed to be the most important. The second way was to use recursive feature elimination with cross-validation to look at removing variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model for feature elimination\n",
    "clf_rf = RandomForestClassifier(max_depth = 100, n_estimators = 64, random_state = 0, class_weight = 'balanced')\n",
    "clf_rf.fit(X_train,Y_train.values.ravel())\n",
    "yhat = clf_rf.predict(X_test)\n",
    "prec = mt.precision_score(Y_test,yhat)\n",
    "auc = roc_auc_score(Y_test,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(runs_data.columns.values)\n",
    "\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "imps = permutation_importance(clf_rf, X_test, Y_test)\n",
    "importances = imps.importances_mean\n",
    "std = imps.importances_std\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "\n",
    "featureorder = []\n",
    "for f in range(X_test.shape[1]):\n",
    "    featureorder.append(features[indices[f]])\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_test.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_test.shape[1]), [features[indices[i]] for i in range(33)],rotation = 90)\n",
    "plt.xlim([-1, X_test.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at this graph we can make some simple eliminations of features that did not help in this model. Likely we would want to keep the first three factors and the last 15 factors in the model and eliminate the rest. This has been done in the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optfeatures = featureorder[:3]\n",
    "optfeatures2 = featureorder[-15:]\n",
    "optfeatures = optfeatures + optfeatures2\n",
    "dataopt = runs_data[optfeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recursive Feature Elimination\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "rfecv = RFECV(estimator=clf_rf, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='precision',\n",
    "              min_features_to_select=min_features_to_select)\n",
    "rfecv.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (precision)\")\n",
    "plt.plot(range(min_features_to_select,\n",
    "               len(rfecv.grid_scores_) + min_features_to_select),\n",
    "         rfecv.grid_scores_)\n",
    "plt.show()\n",
    "for i in range(X_train.shape[1]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfecv.support_[i], rfecv.ranking_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping features from dataset not selected by recursive feature elimination\n",
    "dataopt2 = runs_data.drop(runs_data.columns[[10,15,21,26,28,32]],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building out these new datasets we decided to run the same crossvalidated pieces against them. You can see the results of these below. The first two loops are run on the manually optimized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = [64,75,100,128]\n",
    "for x in trees:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', RandomForestClassifier(max_depth = 5, n_estimators = x, random_state = 0, class_weight = 'balanced')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, dataopt, runs_target, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Trees =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the set of manually optimized data we see that 75 trees has the greatest precision. We are also seeing increased scores for AUC and Precision. There is a jump here from .7794 to .782 and .1734 to .1751 respectively. The model, however, does take slightly longer to run with the larger forest size, but not substantially longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = [5,20,100,None]\n",
    "for x in depth:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', RandomForestClassifier(max_depth = x, n_estimators = 75, random_state = 0, class_weight = 'balanced')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, dataopt, runs_target, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Depth =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results from max_depth, we would want to select a depth max of 100 like in the last model. This is because the difference between precision and AUC is none existant between limiting depth to 100 and no limit. So the runtime benefit here is the deciding factor. Next we will run this again but with the recursive feature elimination dataset to see if there is a difference between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = [64,75,100,128]\n",
    "for x in trees:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', RandomForestClassifier(max_depth = 5, n_estimators = x, random_state = 0, class_weight = 'balanced')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, dataopt2, runs_target, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Trees =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just based on this first run. It doesnt look like the recursive feature elimination will beat out the manual feature selection but for the sake of being thorough we went ahead and ran the loop to look at depth with a forest size of 128 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = [5,20,100,None]\n",
    "for x in depth:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', RandomForestClassifier(max_depth = x, n_estimators = 75, random_state = 0, class_weight = 'balanced')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, dataopt2, runs_target, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Depth =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this loop were surprising as originally it looked as though the RFE dataset would be coming back with lower stats than the manually selected features. However, once the loop for max depth was run there was a jump in statistics from precision .4219 and AUC .7477 to precision .4277 and AUC .7491 at a max depth of 100. This means that our best model for runtime efficiency, precision and AUC would be a Random Forest with weight classed balance, a max depth of 100 and forest size of 75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "Now that we have a best model for the wins of task one we need to move to task two of trying to classify horses that place instead of just the winners from each race. In theory the statistics for these models should all be better due to the fact that the number of posible successes is higher proportionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = [5,20,100,None]\n",
    "for x in depth:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', RandomForestClassifier(max_depth = x, n_estimators = 64, random_state = 0, class_weight = 'balanced')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target2, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Depth =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first run of the data running a with no max depth interestingly seems to be comparable statistics wise with running a model with max depth 100, and strangly it ran faster than the max depth 100. So we will now run the loop again to tune forest size using this max depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = [64,75,100,128]\n",
    "for x in trees:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', RandomForestClassifier(max_depth = None, n_estimators = x, random_state = 0, class_weight = 'balanced')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target2, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Trees =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the second loop our best metrics can be found with a forest size of 128 trees. Now that we have these base statistics. We will want to again optimize the data set using RFE. We decided not to attempt to beat the machine this time and instead utilize RFE to expidite model build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run best model with normal train test split for feature selection\n",
    "cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "# fill in the training and testing data and save as separate variables\n",
    "for trainidx, testidx in cv.split(runs_data,runs_target2):\n",
    "    # note that these are sparse matrices\n",
    "    X_train, X_test = runs_data.iloc[trainidx], runs_data.iloc[testidx] \n",
    "    Y_train, Y_test = runs_target.iloc[trainidx], runs_target.iloc[testidx]\n",
    "\n",
    "mmscl_obj = MinMaxScaler()\n",
    "X_train = mmscl_obj.fit_transform(X_train,y = None)\n",
    "X_test = mmscl_obj.transform(X_test)\n",
    "\n",
    "clf_rf = RandomForestClassifier(max_depth = None, n_estimators = 128, random_state = 0, class_weight = 'balanced')\n",
    "clf_rf.fit(X_train,Y_train.values.ravel())\n",
    "yhat = clf_rf.predict(X_test)\n",
    "prec = mt.precision_score(Y_test,yhat)\n",
    "auc = roc_auc_score(Y_test,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recursive Feature Elimination\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "rfecv = RFECV(estimator=clf_rf, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='precision',\n",
    "              min_features_to_select=min_features_to_select)\n",
    "rfecv.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (precision)\")\n",
    "plt.plot(range(min_features_to_select,\n",
    "               len(rfecv.grid_scores_) + min_features_to_select),\n",
    "         rfecv.grid_scores_)\n",
    "plt.show()\n",
    "for i in range(X_train.shape[1]):\n",
    "    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfecv.support_[i], rfecv.ranking_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping features from dataset not selected by recursive feature elimination\n",
    "dataopt3 = runs_data.drop(runs_data.columns[[8,10,11,14,15,17,18,20,21,23,25,26,28,30,31,32]],axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data set because the number of possible successes goes up, it is not surprising that a smaller number of features is needed to predict this success. We see all the same features eliminated that were eliminated when predicting a winner, but more are added to the list as they are less important in predicting a top 3 placing. Now that we have the optimized dataset for place. We will run the loops again to look for improved performace of both metrics and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = [5,20,100,None]\n",
    "for x in depth:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', RandomForestClassifier(max_depth = x, n_estimators = 64, random_state = 0, class_weight = 'balanced')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, dataopt3, runs_target2, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Depth =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting looking at this data where the depth 5 statistics beat the original non-optimized model, however as depth increased. The optimized model didnt seem to keep pace with the original model.  This might be a similar situation to the last attempt at optimization though where the final model did manage to beat out the original model. From this we will use max depth None to try to optimize our final model. The reason for chosing None of 100 even though 100 had a better calculation speed was to try to compare like for like models here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = [64,75,100,128]\n",
    "for x in trees:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    steps.append(('model', RandomForestClassifier(max_depth = None, n_estimators = x, random_state = 0, class_weight = 'balanced')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "\n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, dataopt3, runs_target2, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When Trees =',x)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final optimized model would have been with a max_depth of None and a forest size of x. Interestingly the optimized model here did not perform better than the full model. This could be because the algorithm for it is trying to balance bias and variability and this time the metrics just didn't increase the precision, but may have made it more adaptable to another data set. However, just based on metrics alone, the full model should be deployed in this case. To compare these to the other types of models we are going to use AUC as it is an easy statistical measure of the predictive power of each model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split won\n",
    "X_train_opt,X_test_opt,Y_train_opt,Y_test_opt = train_test_split(optdata2,runs_target,test_size=0.20,random_state=0)\n",
    "\n",
    "clf_rfw = RandomForestClassifier(max_depth = 100, n_estimators = 64, random_state = 0, class_weight = 'balanced')\n",
    "clf_rfw.fit(X_train_opt,Y_train_opt.values.ravel())\n",
    "yhat = clf_rfw.predict(X_test_opt)\n",
    "prec = mt.precision_score(Y_test_opt,yhat)\n",
    "auc = roc_auc_score(Y_test_opt,yhat)\n",
    "print('When Trees =',x)\n",
    "print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "print(' ')\n",
    "plot_confusion_matrix(clf_rfw, X_test_opt, Y_test_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split won\n",
    "X_train_place,X_test_opt,Y_train_opt,Y_test_opt = train_test_split(optdata2,runs_target,test_size=0.20,random_state=0)\n",
    "\n",
    "clf_rfp = RandomForestClassifier(max_depth = 100, n_estimators = 64, random_state = 0, class_weight = 'balanced')\n",
    "clf_rfp.fit(X_train_opt,Y_train_place.values.ravel())\n",
    "yhat = clf_rfp.predict(X_test_place)\n",
    "prec = mt.precision_score(Y_test_place,yhat)\n",
    "auc = roc_auc_score(Y_test_place,yhat)\n",
    "print('When Trees =',x)\n",
    "print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "print(' ')\n",
    "plot_confusion_matrix(clf_rfp, X_test_place, Y_test_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 5.1.4: Model 4 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Modeling and Evaluation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t• Modeling and Evaluation 4\n",
    "\t\t○ Analyze the results using your chosen method of evaluation. \n",
    "            - Use visualizations of the results to bolster the analysis. \n",
    "            - Explain any visuals and analyze why they are interesting to someone that might use this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 6.1: Analyze the results using your chosen method of evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models to Predict Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models to Predict Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Modeling and Evaluation 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t• Modeling and Evaluation 5\n",
    "\t\t○ Discuss the advantages of each model for each classification task, if any. \n",
    "\t\t\t- If there are not advantages, explain why. \n",
    "\t\t○ Is any model better than another? \n",
    "\t\t○ Is the difference significant with 95% confidence? \n",
    "\t\t\t- Use proper statistical comparison methods. You must use statistical comparison techniques—be sure \n",
    "            they are appropriate for your chosen method of validation as discussed in unit 7 of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 7.1: Discuss the advantages of each model for each classification task, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 7.2: Is any model better than another? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 7.3: Is the difference significant with 95% confidence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Modeling and Evaluation 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t• Modeling and Evaluation 6\n",
    "\t\t○ Which attributes from your analysis are most important? \n",
    "            - Use proper methods discussed in class to evaluate the importance of different attributes. \n",
    "\t\t○ Discuss the results and hypothesize about why certain attributes are more important than others for a\n",
    "        given classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 8.1: Which attributes from your analysis are most important? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 8.1.1: Model 1 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 8.1.2: Model 2 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 8.1.3: Model 3 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 8.1.4: Model 4 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to predict Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t• Deployment\n",
    "\t\t○ How useful is your model for interested parties (i.e., the companies or organizations that might want\n",
    "        to use it for prediction)? \n",
    "\t\t○ How would you measure the model's value if it was used by these parties? \n",
    "\t\t○ How would your deploy your model for interested parties? \n",
    "\t\t○ What other data should be collected? \n",
    "\t\t○ How often would the model need to be updated, etc.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 9.1: How useful is your model for interested parties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 9.2: How would you measure the model's value if it was used by these parties? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 9.3: How would your deploy your model for interested parties? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 9.4: What other data should be collected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 9.5: How often would the model need to be updated? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t• Exceptional Work\n",
    "\t\t○ You have free reign to provide additional analyses. \n",
    "\t\t\t- One idea: grid search parameters in a parallelized fashion and visualize the performances across\n",
    "            attributes. \n",
    "\t\t\t- Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 10.1: Recursive Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 10.2: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Section 10.3: Extra Model to predict Won and Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line break between reports\n",
    "========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on how to scale, model, and get metrics while using cross validation with a pipeline\n",
    "## Example also shows how to use this to tune a tune a hyper-parameter\n",
    "\n",
    "#### Possible adjustments you should look at\n",
    "- The parameter being hypertuned and values at the start of the loop\n",
    "- Is the scaler type what you want\n",
    "- Is the model information updated\n",
    "- The cross_validate() function uses runs_data and runs_target to check for win. You need to unpdate to runs_target2 to check for show\n",
    "- You should also update the first print statement to reflect your hyperparameter name\n",
    "- This should be applied to all models so we are consistent because original method was not fully utilizing all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referenced website: https://machinelearningmastery.com/data-preparation-without-data-leakage/\n",
    "Kvals = [5,10]\n",
    "for k in Kvals:\n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "    #steps.append(('model', KNeighborsClassifier(n_neighbors=k, weights='distance', metric='euclidean')))\n",
    "    steps.append(('model', KNeighborsClassifier(n_neighbors=k, weights='distance', metric='euclidean')))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    # define the evaluation procedure\n",
    "    #cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8, random_state = 0)\n",
    "    cv = StratifiedKFold(n_splits = 10, shuffle=True, random_state = 0)\n",
    "\n",
    "    \n",
    "    #Modeling with pipeline and scoring with cross_validate()\n",
    "    scores = cross_validate(pipeline, runs_data, runs_target, scoring=['roc_auc','accuracy','precision'],\n",
    "                             cv=cv, return_train_score=False)\n",
    "    print('When k =',k)\n",
    "    print('   Mean Precision: ',np.mean(scores['test_precision']))\n",
    "    print('   Mean AUC: ',np.mean(scores['test_roc_auc']))\n",
    "    print('   Mean Accuracy: ',np.mean(scores['test_accuracy']))\n",
    "    print('   Mean Time per Split: ',np.mean(scores['fit_time'])+np.mean(scores['score_time']))\n",
    "    print(' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
